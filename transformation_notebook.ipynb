{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # One time run to mount the Azure storage account to Databricks file system\n",
    "\n",
    "# # Getting credentials from Azure Key Vault through Databricks secret scope\n",
    "# client_id = dbutils.secrets.get(scope=\"adzuna-project-db-secret-scope\",key=\"adzuna-project-app-client-id\")\n",
    "# client_secret = dbutils.secrets.get(scope=\"adzuna-project-db-secret-scope\",key=\"adzuna-project-app-client-secret\")\n",
    "# tenant_id = dbutils.secrets.get(scope=\"adzuna-project-db-secret-scope\",key=\"adzuna-project-app-tenant-id\")\n",
    "# client_endpoint = f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n",
    "\n",
    "# # Mounting the storage account\n",
    "# configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "# \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "# \"fs.azure.account.oauth2.client.id\": client_id,\n",
    "# \"fs.azure.account.oauth2.client.secret\": client_secret,\n",
    "# \"fs.azure.account.oauth2.client.endpoint\": client_endpoint}\n",
    "\n",
    "# dbutils.fs.mount(\n",
    "#     source = \"abfss://raw-data@saadzunaproject.dfs.core.windows.net\", # contrainer@storageacc\n",
    "#     mount_point = \"/mnt/adzuna_data\",\n",
    "#     extra_configs = configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL pipeline code\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col, explode, current_timestamp, to_date\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create the Spark session\n",
    "spark = SparkSession.builder.appName(\"adzuna-project\").getOrCreate()\n",
    "\n",
    "# Define paths for raw data, staging table, and final (mart) table\n",
    "raw_data_path = \"/mnt/adzuna_data/json/\"\n",
    "staging_path = \"/mnt/delta/tables/staging/adzuna_jobs\"\n",
    "final_path = \"/mnt/delta/tables/final/adzuna_jobs\"\n",
    "\n",
    "# ============================================================\n",
    "# PART 1: Ingest Raw Data and Merge into Staging with elt_updated (timestamp)\n",
    "# ============================================================\n",
    "\n",
    "# Read raw JSON data from ADLS\n",
    "adzuna_jobs_raw = spark.read.format(\"json\").load(raw_data_path)\n",
    "\n",
    "# Transform raw data:\n",
    "#   - Explode the \"items\" array\n",
    "#   - Select desired fields\n",
    "#   - Drop duplicates based on \"id\"\n",
    "#   - Add the elt_updated column as current_timestamp()\n",
    "adzuna_jobs_batch = adzuna_jobs_raw.withColumn(\"item\", explode(\"items\")) \\\n",
    "    .select(\n",
    "        col(\"item.id\").alias(\"id\"),\n",
    "        col(\"item.title\").alias(\"title\"),\n",
    "        col(\"item.location.display_name\").alias(\"location_display_name\"),\n",
    "        col(\"item.company.display_name\").alias(\"company_display_name\"),\n",
    "        col(\"item.category.label\").alias(\"category_label\"),\n",
    "        col(\"item.description\").alias(\"description\"),\n",
    "        col(\"item.redirect_url\").alias(\"redirect_url\"),\n",
    "        col(\"item.created\").alias(\"created\")\n",
    "    ).dropDuplicates([\"id\"]) \\\n",
    "    .withColumn(\"elt_updated\", current_timestamp())\n",
    "\n",
    "# Merge (upsert) the new batch into the staging Delta table.\n",
    "# When a record is updated or inserted, its elt_updated is set to current_timestamp()\n",
    "if DeltaTable.isDeltaTable(spark, staging_path):\n",
    "    stagingDeltaTable = DeltaTable.forPath(spark, staging_path)\n",
    "    stagingDeltaTable.alias(\"t\").merge(\n",
    "        adzuna_jobs_batch.alias(\"s\"),\n",
    "        \"t.id = s.id\"\n",
    "    ).whenMatchedUpdate(set={\n",
    "        \"title\": \"s.title\",\n",
    "        \"location_display_name\": \"s.location_display_name\",\n",
    "        \"company_display_name\": \"s.company_display_name\",\n",
    "        \"category_label\": \"s.category_label\",\n",
    "        \"description\": \"s.description\",\n",
    "        \"redirect_url\": \"s.redirect_url\",\n",
    "        \"created\": \"s.created\",\n",
    "        \"elt_updated\": \"current_timestamp()\"\n",
    "    }).whenNotMatchedInsert(values={\n",
    "        \"id\": \"s.id\",\n",
    "        \"title\": \"s.title\",\n",
    "        \"location_display_name\": \"s.location_display_name\",\n",
    "        \"company_display_name\": \"s.company_display_name\",\n",
    "        \"category_label\": \"s.category_label\",\n",
    "        \"description\": \"s.description\",\n",
    "        \"redirect_url\": \"s.redirect_url\",\n",
    "        \"created\": \"s.created\",\n",
    "        \"elt_updated\": \"current_timestamp()\"\n",
    "    }).execute()\n",
    "else:\n",
    "    # If the staging table does not exist, create it.\n",
    "    adzuna_jobs_batch.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(staging_path)\n",
    "\n",
    "# ============================================================\n",
    "# PART 2: Incrementally Update the Final Table from Staging\n",
    "#         (Process only records where elt_updated > max(elt_updated) in final)\n",
    "# ============================================================\n",
    "\n",
    "# Determine the maximum elt_updated value from the final table (if it exists)\n",
    "try:\n",
    "    final_df = spark.read.format(\"delta\").load(final_path)\n",
    "    max_elt_final = final_df.agg(F.max(\"elt_updated\").alias(\"max_elt\")).collect()[0][\"max_elt\"]\n",
    "except Exception as e:\n",
    "    max_elt_final = None\n",
    "\n",
    "# Read the entire staging Delta table\n",
    "staging_df = spark.read.format(\"delta\").load(staging_path)\n",
    "\n",
    "# Filter staging records:\n",
    "#   - If final table is empty, process all records.\n",
    "#   - Otherwise, process only records where elt_updated > max_elt_final.\n",
    "if max_elt_final:\n",
    "    staging_incremental = staging_df.filter(col(\"elt_updated\") > F.lit(max_elt_final))\n",
    "else:\n",
    "    staging_incremental = staging_df\n",
    "\n",
    "# Transform the incremental staging records to the final (mart) schema.\n",
    "# Rename columns and add a partition column based on job_created date.\n",
    "final_incremental = staging_incremental \\\n",
    "    .withColumnRenamed(\"id\", \"job_id\") \\\n",
    "    .withColumnRenamed(\"title\", \"job_title\") \\\n",
    "    .withColumnRenamed(\"location_display_name\", \"job_location\") \\\n",
    "    .withColumnRenamed(\"company_display_name\", \"company_name\") \\\n",
    "    .withColumnRenamed(\"category_label\", \"job_category\") \\\n",
    "    .withColumnRenamed(\"description\", \"job_description\") \\\n",
    "    .withColumnRenamed(\"redirect_url\", \"job_url\") \\\n",
    "    .withColumnRenamed(\"created\", \"job_created\") \\\n",
    "    .withColumn(\"job_created\", col(\"job_created\").cast(\"timestamp\")) \\\n",
    "    .withColumn(\"job_created_date\", to_date(\"job_created\")) \\\n",
    "    .withColumn(\"elt_updated\", col(\"elt_updated\")) \\\n",
    "    .dropDuplicates([\"job_id\"])\n",
    "\n",
    "# Merge (upsert) the incremental final data into the final Delta table\n",
    "if DeltaTable.isDeltaTable(spark, final_path):\n",
    "    finalDeltaTable = DeltaTable.forPath(spark, final_path)\n",
    "    finalDeltaTable.alias(\"t\").merge(\n",
    "        final_incremental.alias(\"s\"),\n",
    "        \"t.job_id = s.job_id\"\n",
    "    ).whenMatchedUpdate(set={\n",
    "        \"job_title\": \"s.job_title\",\n",
    "        \"job_location\": \"s.job_location\",\n",
    "        \"company_name\": \"s.company_name\",\n",
    "        \"job_category\": \"s.job_category\",\n",
    "        \"job_description\": \"s.job_description\",\n",
    "        \"job_url\": \"s.job_url\",\n",
    "        \"job_created\": \"s.job_created\",\n",
    "        \"job_created_date\": \"s.job_created_date\",\n",
    "        \"elt_updated\": \"s.elt_updated\"\n",
    "    }).whenNotMatchedInsert(values={\n",
    "        \"job_id\": \"s.job_id\",\n",
    "        \"job_title\": \"s.job_title\",\n",
    "        \"job_location\": \"s.job_location\",\n",
    "        \"company_name\": \"s.company_name\",\n",
    "        \"job_category\": \"s.job_category\",\n",
    "        \"job_description\": \"s.job_description\",\n",
    "        \"job_url\": \"s.job_url\",\n",
    "        \"job_created\": \"s.job_created\",\n",
    "        \"job_created_date\": \"s.job_created_date\",\n",
    "        \"elt_updated\": \"s.elt_updated\"\n",
    "    }).execute()\n",
    "else:\n",
    "    final_incremental.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .partitionBy(\"job_created_date\") \\\n",
    "        .save(final_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating stg_adzuna_jobs and adzuna_jobs SQL tables\n",
    "# Execute only once!\n",
    "\n",
    "# %sql\n",
    "# CREATE TABLE IF NOT EXISTS stg_adzuna_jobs\n",
    "# USING DELTA\n",
    "# LOCATION '/mnt/delta/tables/staging/adzuna_jobs';\n",
    "\n",
    "# CREATE TABLE IF NOT EXISTS adzuna_jobs\n",
    "# USING DELTA\n",
    "# LOCATION '/mnt/delta/tables/final/adzuna_jobs';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful functions for debugging\n",
    "\n",
    "# dbutils.fs.rm('/mnt/delta/tables/staging/adzuna_jobs', True)\n",
    "# dbutils.fs.rm('/mnt/delta/tables/final/adzuna_jobs', True)\n",
    "\n",
    "#%fs ls '/mnt/delta/tables/final/adzuna_jobs'\n",
    "\n",
    "# %sql\n",
    "# select * from stg_adzuna_jobs limit 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
